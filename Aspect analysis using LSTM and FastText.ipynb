{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "7_My. Аспектный сентимент-анализ как NER.ipynb\"",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d94hv8yZkIlL"
      },
      "source": [
        "# Aspect analysis using LSTM and FastText\r\n",
        "### Based on course [\"Нейронные сети и обработка текста\"](https://stepik.org/course/54098/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vHv5kTMltqF"
      },
      "source": [
        "## Required libraries, functions and classes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moOJ8l-lkIlQ"
      },
      "source": [
        "!pip3 install livelossplot --quiet\n",
        "!pip3 install ipymarkup --quiet\n",
        "!pip3 install pytorch-nlp --quiet\n",
        "!pip3 install --upgrade gensim --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTgIFG87lhg4"
      },
      "source": [
        "from google.colab import drive\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\r\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\r\n",
        "from torch import linalg as LA\r\n",
        "\r\n",
        "import zipfile\r\n",
        "\r\n",
        "import gensim\r\n",
        "\r\n",
        "import re\r\n",
        "\r\n",
        "from nltk.tokenize import RegexpTokenizer\r\n",
        "\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "\r\n",
        "import datetime\r\n",
        "\r\n",
        "from copy import deepcopy\r\n",
        "\r\n",
        "import lxml\r\n",
        "from lxml import etree\r\n",
        "\r\n",
        "from traceback import format_exc\r\n",
        "\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "from sklearn.utils.multiclass import unique_labels\r\n",
        "\r\n",
        "from collections import Counter\r\n",
        "\r\n",
        "from pprint import pprint\r\n",
        "\r\n",
        "from collections import defaultdict\r\n",
        "\r\n",
        "from livelossplot import PlotLosses\r\n",
        "\r\n",
        "from ipymarkup import show_box_markup\r\n",
        "from ipymarkup.palette import PALETTE, palette, BLUE, RED, GREEN, PURPLE, BROWN, ORANGE\r\n",
        "\r\n",
        "# aspect\r\n",
        "PALETTE.set('Comfort', ORANGE)\r\n",
        "PALETTE.set('B-Comfort', ORANGE)\r\n",
        "PALETTE.set('I-Comfort', ORANGE)\r\n",
        "PALETTE.set('Appearance', BLUE)\r\n",
        "PALETTE.set('B-Appearance', BLUE)\r\n",
        "PALETTE.set('I-Appearance', BLUE)\r\n",
        "PALETTE.set('Reliability', GREEN)\r\n",
        "PALETTE.set('B-Reliability', GREEN)\r\n",
        "PALETTE.set('I-Reliability', GREEN)\r\n",
        "PALETTE.set('Safety', PURPLE)\r\n",
        "PALETTE.set('B-Safety', PURPLE)\r\n",
        "PALETTE.set('I-Safety', PURPLE)\r\n",
        "PALETTE.set('Driveability', BLUE)\r\n",
        "PALETTE.set('B-Driveability', BLUE)\r\n",
        "PALETTE.set('I-Driveability', BLUE)\r\n",
        "PALETTE.set('Whole', RED)\r\n",
        "PALETTE.set('B-Whole', RED)\r\n",
        "PALETTE.set('I-Whole', RED)\r\n",
        "PALETTE.set('Costs', RED)\r\n",
        "PALETTE.set('B-Costs', RED)\r\n",
        "PALETTE.set('I-Costs', RED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7FkkidjlmOR"
      },
      "source": [
        "def init_random_seed(value=0):\r\n",
        "    np.random.seed(value)\r\n",
        "    torch.manual_seed(value)\r\n",
        "    torch.cuda.manual_seed(value)\r\n",
        "    torch.backends.cudnn.deterministic = True\r\n",
        "\r\n",
        "init_random_seed()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFKvXjb1A--t"
      },
      "source": [
        "def divisors(n):\r\n",
        "    i = 1\r\n",
        "    divisors = []\r\n",
        "    while i <= n**0.5:\r\n",
        "        if (n % i == 0) : \r\n",
        "            if (n / i == i):\r\n",
        "                divisors.append(i)\r\n",
        "            else:\r\n",
        "                divisors.extend([i, n // i])\r\n",
        "        i = i + 1\r\n",
        "    return sorted(divisors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XnyEADpA_co"
      },
      "source": [
        "def copy_data_to_device(data, device):\r\n",
        "    if torch.is_tensor(data):\r\n",
        "        return data.to(device)\r\n",
        "    elif isinstance(data, (list, tuple)):\r\n",
        "        return [copy_data_to_device(elem, device) for elem in data]\r\n",
        "    raise ValueError('Invalid data type {}'.format(type(data)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhHcBBcEmfzw"
      },
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, classes, figsize=(7, 7), normalize=False, title=None, cmap=plt.cm.Blues):\r\n",
        "   \r\n",
        "    cm = confusion_matrix(y_true, y_pred, classes)\r\n",
        "    if normalize:\r\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\r\n",
        "    \r\n",
        "    fig, ax = plt.subplots(figsize=figsize)\r\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\r\n",
        "   \r\n",
        "    ax.figure.colorbar(im, ax=ax)\r\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\r\n",
        "           yticks=np.arange(cm.shape[0]),\r\n",
        "           xticklabels=classes, \r\n",
        "           yticklabels=classes,\r\n",
        "           title=title,\r\n",
        "           ylabel='True tag',\r\n",
        "           xlabel=\"Predicted tag\")\r\n",
        "    \r\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",rotation_mode=\"anchor\")\r\n",
        "\r\n",
        "    fmt = '.2f' if normalize else 'd'\r\n",
        "    thresh = cm.max() / 2.\r\n",
        "    for i in range(cm.shape[0]):\r\n",
        "        for j in range(cm.shape[1]):\r\n",
        "            ax.text(j, i, format(cm[i, j], fmt), ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\r\n",
        "    fig.tight_layout()\r\n",
        "    \r\n",
        "    return ax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjmH2rqBvhLx"
      },
      "source": [
        "def get_sentiment_spans(root):\r\n",
        "    return get_tag_spans(root,'sentiment')\r\n",
        "\r\n",
        "def get_aspect_spans(root):\r\n",
        "    return get_tag_spans(root,'category')\r\n",
        "\r\n",
        "def get_tag_spans(root, tag):\r\n",
        "    attributes = root.attrib        \r\n",
        "    start = int(attributes['from'])\r\n",
        "    end   = int(attributes['to'])\r\n",
        "    tag   = attributes[tag]\r\n",
        "    span  = (start, end, tag)\r\n",
        "    return span\r\n",
        "\r\n",
        "\r\n",
        "def parse_xml_sentiment(xml_file): return parseXML(xml_file, get_sentiment_spans)\r\n",
        "def parse_xml_aspect(xml_file):    return parseXML(xml_file, get_aspect_spans)\r\n",
        "\r\n",
        "\r\n",
        "def parseXML(xmlFile, span_func):\r\n",
        "    xml  = open(xmlFile).read()\r\n",
        "    root = etree.fromstring(xml)\r\n",
        "    \r\n",
        "    result = []\r\n",
        "    for review in root.getchildren(): \r\n",
        "        for elem in review.getchildren():            \r\n",
        "            if elem.tag == 'text':\r\n",
        "                text  = elem.text\r\n",
        "            if elem.tag == 'aspects':\r\n",
        "                spans = [span_func(xml_span) for xml_span in elem.getchildren()]\r\n",
        "                spans = span_sanity_filter(spans)          \r\n",
        "        result.append((text, spans))\r\n",
        "    return result\r\n",
        "\r\n",
        "\r\n",
        "def span_sanity_filter(spans):\r\n",
        "    result = [spans[0]]\r\n",
        "    \r\n",
        "    for span in spans[1:]:\r\n",
        "        _, prev_span_end, _ = result[-1]\r\n",
        "        curr_span_start, _, _ = span\r\n",
        "        if prev_span_end < curr_span_start:\r\n",
        "            result.append(span)\r\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMDHUXKzFZtp"
      },
      "source": [
        "def fill_gaps(text, source_spans):\r\n",
        "    \"\"\" Add 'Other' tag \"\"\"\r\n",
        "    chunks = []\r\n",
        "    text_pos = 0\r\n",
        "\r\n",
        "    for span in source_spans:\r\n",
        "        s, e, t = span\r\n",
        "        if text_pos < s:\r\n",
        "            chunks.append((text_pos, s, 'Other'))\r\n",
        "        chunks.append((s, e, t))\r\n",
        "        text_pos = e\r\n",
        "    if text_pos < len(text) - 1:\r\n",
        "        chunks.append((text_pos, len(text), 'Other'))\r\n",
        "\r\n",
        "    return chunks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO3YC_iUE-u1"
      },
      "source": [
        "def regex_sentence_detector(text):\r\n",
        "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<!\\b\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|!)\\s', text)\r\n",
        "    return sentences\r\n",
        "\r\n",
        "def sentence_splitter(text, spans):\r\n",
        "    result = []\r\n",
        "    \r\n",
        "    sentences = regex_sentence_detector(text)\r\n",
        "    \r\n",
        "    sent_start, span_idx = 0, 0\r\n",
        "    \r\n",
        "    for sent in sentences:\r\n",
        "        sent_end = sent_start + len(sent)\r\n",
        "        sent_spans = []\r\n",
        "        for span in spans[span_idx:]:\r\n",
        "            s, e, t = span\r\n",
        "            if e <= sent_end:\r\n",
        "                sent_spans.append((s - sent_start, e - sent_start, t))\r\n",
        "                span_idx += 1\r\n",
        "            else:\r\n",
        "                continue\r\n",
        "        result.append((text[sent_start:sent_end], sent_spans))\r\n",
        "        sent_start = 1 + sent_end\r\n",
        "\r\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir8_XomlGoUD"
      },
      "source": [
        "def extract_BIO_tagged_tokens(text, source_spans, tokenizer):\r\n",
        "    tokens_w_tags = []\r\n",
        "    for span in source_spans:\r\n",
        "        s, e, tag = span\r\n",
        "        tokens = tokenizer(text[s:e])\r\n",
        "        if tag == 'Other':\r\n",
        "            tokens_w_tags += [(token, tag) for token in tokens]\r\n",
        "        else:\r\n",
        "            tokens_w_tags.append((tokens[0],'B-' + tag))\r\n",
        "            for token in tokens[1:]:\r\n",
        "                tokens_w_tags.append((token,'I-' + tag))\r\n",
        "    return tokens_w_tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QyWzml9BH8I"
      },
      "source": [
        "def prepare_data(texts_with_spans, tokenize_func):\r\n",
        "    result = []\r\n",
        "    for text, spans in texts_with_spans:\r\n",
        "        for sent, sent_spans in sentence_splitter(text, spans):\r\n",
        "            if len(sent) > 1:\r\n",
        "                cover_spans = fill_gaps(sent, sent_spans)                \r\n",
        "                try:\r\n",
        "                    tokens_w_biotags = extract_BIO_tagged_tokens(sent, cover_spans, tokenize_func)\r\n",
        "                    tokens, biotags = list(zip(*tokens_w_biotags))\r\n",
        "                    result.append((tokens, biotags))\r\n",
        "                except Exception as e:\r\n",
        "                    continue\r\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9QnFPCPDYFJ"
      },
      "source": [
        "def show_markup(sentence, tags, palette_set):\r\n",
        "    mapper = lambda tag: tag[2:] if tag!='Other' else tag\r\n",
        "    \r\n",
        "    tags = [mapper(tag) for tag in tags]\r\n",
        "    text = ' '.join(sentence)\r\n",
        "    spans = []\r\n",
        "        \r\n",
        "    start, end, tag = 0, len(sentence[0]), tags[0]\r\n",
        "    \r\n",
        "    for word, ttag in zip(sentence[1:], tags[1:]): \r\n",
        "        if tag == ttag:\r\n",
        "            end  += 1 + len(word)\r\n",
        "        else:\r\n",
        "            span  = (start, end, tag)\r\n",
        "            spans.append(span)\r\n",
        "            start = 1 + end\r\n",
        "            end += 1 + len(word)\r\n",
        "            tag = ttag\r\n",
        "            \r\n",
        "    span = (start, end, tag)\r\n",
        "    spans.append(span)        \r\n",
        "            \r\n",
        "    show_box_markup(text, spans, palette=palette_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcqL7wWS55ak"
      },
      "source": [
        "def form_vocabulary_and_tagset(words_w_tags):\r\n",
        "    dictionary = defaultdict(Counter)\r\n",
        "    for words, tags in words_w_tags: \r\n",
        "        for word, tag in zip(words, tags):\r\n",
        "            dictionary[tag].update([word])      \r\n",
        "    return dictionary, set(dictionary.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFNdLDk57BfM"
      },
      "source": [
        "class TagConverter():\r\n",
        "    def __init__(self, tags):\r\n",
        "        self.idx_to_tag = sorted(tags)\r\n",
        "        self.tag_to_idx = {tag: idx + 1 for idx, tag in enumerate(self.idx_to_tag)}\r\n",
        "\r\n",
        "    def tags_to_index(self, words):\r\n",
        "        return torch.tensor([self.tag_to_idx[word] for word in words], dtype=torch.long)\r\n",
        "    \r\n",
        "    def indices_to_tags(self, indices):\r\n",
        "        return [self.idx_to_tag[idx - 1] for idx in indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD-86JWd9bCO"
      },
      "source": [
        "class WordConverter():\r\n",
        "    def __init__(self, w2v_model):\r\n",
        "        self.w2v_model = w2v_model\r\n",
        "        self.emb_size = w2v_model.vector_size\r\n",
        "\r\n",
        "    def convert(self, words):\r\n",
        "        embeddings = [self.w2v_model.get_vector(word) for word in words]\r\n",
        "        embeddings = torch.FloatTensor(embeddings)\r\n",
        "        return embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YKa1NM6nPvF"
      },
      "source": [
        "class TaggerDataset(Dataset):\r\n",
        "    def __init__(self, sample, word_converter, tag_converter):\r\n",
        "        self.words, self.tags = map(list, zip(*sample))\r\n",
        "        self.max_len = max(map(len, self.words))\r\n",
        "        self.word_converter, self.tag_converter = word_converter, tag_converter\r\n",
        "        \r\n",
        "    def __len__(self):\r\n",
        "        return len(self.words)\r\n",
        "\r\n",
        "    def __getitem__(self, item):\r\n",
        "        sentence_embeddings = torch.zeros((self.max_len, word_converter.emb_size))\r\n",
        "        sentence_tags = torch.zeros((self.max_len), dtype=torch.long)\r\n",
        "        sentence_len = len(self.words[item])\r\n",
        "        sentence_embeddings[:sentence_len, :] = self.word_converter.convert(self.words[item])\r\n",
        "        sentence_tags[:sentence_len] = self.tag_converter.tags_to_index(self.tags[item])\r\n",
        "        return (item, sentence_embeddings, sentence_len), sentence_tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKYfPMwyNvPW"
      },
      "source": [
        "class LSTMTagger(nn.Module):\r\n",
        "    def __init__(self, tagset_size, embedding_dim=300, hidden_dim=300, num_layers=1, layer_dropout=0, emb_dropout=0):\r\n",
        "        super(self.__class__, self).__init__()\r\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, dropout=layer_dropout, bidirectional=True)\r\n",
        "        self.hidden_to_tag = nn.Linear(2*hidden_dim, tagset_size)\r\n",
        "        self.emb_dropout = nn.Dropout(emb_dropout)\r\n",
        "\r\n",
        "    def forward(self, batch_x):\r\n",
        "        _, words_embs, seq_lengths = batch_x\r\n",
        "        words_embs = self.emb_dropout(words_embs)\r\n",
        "        packed_words_embs = pack_padded_sequence(words_embs, seq_lengths.cpu(), batch_first=True, enforce_sorted=False)\r\n",
        "        lstm_out, _ = self.lstm(packed_words_embs)\r\n",
        "        output, input_sizes = pad_packed_sequence(lstm_out, batch_first=True, total_length=words_embs.shape[1])\r\n",
        "        tag_space = self.hidden_to_tag(output)\r\n",
        "        tag_scores  = F.log_softmax(tag_space, dim=-1)\r\n",
        "        return tag_scores\r\n",
        "    \r\n",
        "    def predict_tags(self, words):\r\n",
        "        model.eval()\r\n",
        "        with torch.no_grad():\r\n",
        "            tags_pred = model(words).cpu().numpy()\r\n",
        "            tags_pred = np.argmax(tags_pred, axis=-1)\r\n",
        "        return tags_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFR8oFp2s2GF"
      },
      "source": [
        "def batch_nllloss(pred, target, weights=None):\r\n",
        "    \"\"\"\r\n",
        "    pred - BatchSize x TargetLen x VocabSize\r\n",
        "    target - BatchSize x TargetLen\r\n",
        "    \"\"\"\r\n",
        "    pred_flat = pred.view(-1, pred.shape[-1])\r\n",
        "    target_flat = target.view(-1)\r\n",
        "    return F.nll_loss(pred_flat, target_flat, weight=weights, ignore_index=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBG5We9PyXB5"
      },
      "source": [
        "def train_eval_loop(model, train_dataset, val_dataset, criterion, weights=None, lr=1e-3, epoch_n=100, batch_size_train=32,\r\n",
        "                    batch_size_val=32, device=None, early_stopping_patience=10, l2_reg_alpha=0, data_loader_ctor=DataLoader,\r\n",
        "                    optimizer_ctor=None, lr_scheduler_ctor=None, dataloader_workers_n=0, draw_loss=False, show_bar=False, show_lr=False):\r\n",
        "\r\n",
        "    if device is None:\r\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n",
        "    device = torch.device(device)\r\n",
        "    model.to(device)\r\n",
        "\r\n",
        "    if optimizer_ctor is None:\r\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_reg_alpha)\r\n",
        "    else:\r\n",
        "        optimizer = optimizer_ctor(model.parameters(), lr=lr)\r\n",
        "    \r\n",
        "    if lr_scheduler_ctor is not None:\r\n",
        "        lr_scheduler = lr_scheduler_ctor(optimizer)\r\n",
        "    else:\r\n",
        "        lr_scheduler = None\r\n",
        "    \r\n",
        "    if weights is not None:\r\n",
        "        weights = copy_data_to_device(weights, device)\r\n",
        "\r\n",
        "    if draw_loss:\r\n",
        "        liveplot = PlotLosses()\r\n",
        "\r\n",
        "    train_dataloader = data_loader_ctor(train_dataset, batch_size=batch_size_train, num_workers=dataloader_workers_n)\r\n",
        "    val_dataloader = data_loader_ctor(val_dataset, batch_size=batch_size_val, num_workers=dataloader_workers_n)\r\n",
        "\r\n",
        "    best_val_loss = float(\"inf\")\r\n",
        "    best_epoch_i = 0\r\n",
        "    best_model = deepcopy(model)\r\n",
        "    \r\n",
        "    for epoch_i in range(epoch_n):\r\n",
        "        try:\r\n",
        "            if not draw_loss:\r\n",
        "                epoch_start = datetime.datetime.now()\r\n",
        "                print(f\"Epoch {epoch_i}\")\r\n",
        "\r\n",
        "            model.train()\r\n",
        "            mean_train_loss = 0\r\n",
        "            train_batches_n = 0\r\n",
        "\r\n",
        "            for batch_x, batch_y in tqdm(train_dataloader) if show_bar else train_dataloader:\r\n",
        "                batch_x = copy_data_to_device(batch_x, device)\r\n",
        "                batch_y = copy_data_to_device(batch_y, device)\r\n",
        "                pred = model(batch_x)\r\n",
        "                loss = criterion(pred, batch_y, weights=weights)\r\n",
        "\r\n",
        "                model.zero_grad()\r\n",
        "                loss.backward()\r\n",
        "\r\n",
        "                optimizer.step()\r\n",
        "\r\n",
        "                mean_train_loss += float(loss)\r\n",
        "                train_batches_n += 1\r\n",
        "\r\n",
        "            mean_train_loss /= train_batches_n\r\n",
        "\r\n",
        "            model.eval()\r\n",
        "            mean_val_loss = 0\r\n",
        "            val_batches_n = 0\r\n",
        "\r\n",
        "            with torch.no_grad():\r\n",
        "                for batch_x, batch_y in tqdm(val_dataloader) if show_bar else val_dataloader:\r\n",
        "\r\n",
        "                    batch_x = copy_data_to_device(batch_x, device)\r\n",
        "                    batch_y = copy_data_to_device(batch_y, device)\r\n",
        "\r\n",
        "                    pred = model(batch_x)\r\n",
        "                    loss = criterion(pred, batch_y, weights=weights)\r\n",
        "\r\n",
        "                    mean_val_loss += float(loss)\r\n",
        "                    val_batches_n += 1\r\n",
        "\r\n",
        "            mean_val_loss /= val_batches_n\r\n",
        "            \r\n",
        "            if not draw_loss:\r\n",
        "                print('{} iterations for training and {} for validation, {:0.2f} sec'.format(train_batches_n, val_batches_n,\r\n",
        "                                                           (datetime.datetime.now() - epoch_start).total_seconds()))\r\n",
        "                print('Average value of the train loss function:', mean_train_loss)\r\n",
        "                print('Average value of the validation loss function:', mean_val_loss)\r\n",
        "\r\n",
        "            if draw_loss:\r\n",
        "                liveplot.update({'mean loss': mean_train_loss, \"val_mean loss\": mean_val_loss})\r\n",
        "                liveplot.draw()\r\n",
        "\r\n",
        "            if mean_val_loss < best_val_loss:\r\n",
        "                best_epoch_i = epoch_i\r\n",
        "                best_val_loss = mean_val_loss\r\n",
        "                best_model = deepcopy(model)\r\n",
        "                if not draw_loss:\r\n",
        "                    print('New best model!')\r\n",
        "            elif epoch_i - best_epoch_i > early_stopping_patience:\r\n",
        "                print('The model has not improved over the last {} epochs, stop training'.format(\r\n",
        "                    early_stopping_patience))\r\n",
        "                break\r\n",
        "  \r\n",
        "            if lr_scheduler is not None:\r\n",
        "                if isinstance(lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\r\n",
        "                    lr_scheduler.step(mean_val_loss)\r\n",
        "                elif isinstance(lr_scheduler, torch.optim.lr_scheduler.StepLR):\r\n",
        "                    lr_scheduler.step()\r\n",
        "                    if show_lr:\r\n",
        "                        print(optimizer.param_groups[0]['lr'])\r\n",
        "                else:\r\n",
        "                    lr_scheduler.step()\r\n",
        "\r\n",
        "            print()\r\n",
        "        except KeyboardInterrupt:\r\n",
        "            print('Stopped early by user')\r\n",
        "            break\r\n",
        "        except Exception as ex:\r\n",
        "            print('Error while training: {}\\n{}'.format(ex, format_exc()))\r\n",
        "            break\r\n",
        "\r\n",
        "    return best_val_loss, best_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyvJTZxr0dYX"
      },
      "source": [
        "def predict_tags(model, dataset, device=None, batch_size=None, dataloader_workers_n=5):\r\n",
        "    if batch_size is None:\r\n",
        "        batch_size = len(dataset)\r\n",
        "    if device is None:\r\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n",
        "    results_by_batch = []\r\n",
        "    device = torch.device(device)\r\n",
        "    model.to(device)\r\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=dataloader_workers_n)\r\n",
        "    for batch_x, batch_y in dataloader:\r\n",
        "        batch_x = copy_data_to_device(batch_x, device)                    \r\n",
        "        encoded_tags = model.predict_tags(batch_x)\r\n",
        "        encoded_tags_no_padding = [list(tags_for_one_sentence[:len_of_sentence]) for tags_for_one_sentence, len_of_sentence in zip(encoded_tags, batch_x[-1].cpu())]\r\n",
        "        results_by_batch.extend(encoded_tags_no_padding)\r\n",
        "\r\n",
        "    return results_by_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yaf23DgqDZBm"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lunKPe3CM5Rl"
      },
      "source": [
        "## Loading [data](https://github.com/Samsung-IT-Academy/stepik-dl-nlp/tree/master/datasets/sentirueval2015) & preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkCt1Oi4DeKh"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9LPDMxODjVN"
      },
      "source": [
        "xml_train = \"/content/gdrive/My Drive/ML/datasets/Sentirueval2015/SentiRuEval_car_markup_train.xml\"\r\n",
        "xml_test = \"/content/gdrive/My Drive/ML/datasets/Sentirueval2015/SentiRuEval_car_markup_test.xml\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9pdoG1rkIlS"
      },
      "source": [
        "## BIO-tagging:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_32Rd_UkIlT"
      },
      "source": [
        "word_tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NG0WFJZkIlU"
      },
      "source": [
        "texts_w_aspect_spans = parse_xml_aspect(xml_train)\n",
        "texts_w_aspect_spans = [(text.lower(), spans) for text, spans in texts_w_aspect_spans]\n",
        "train_data = prepare_data(texts_w_aspect_spans, word_tokenizer.tokenize)\n",
        "\n",
        "texts_w_aspect_spans = parse_xml_aspect(xml_test)\n",
        "texts_w_aspect_spans = [(text.lower(), spans) for text, spans in texts_w_aspect_spans]\n",
        "test_data = prepare_data(texts_w_aspect_spans, word_tokenizer.tokenize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMeXcinMkIlV"
      },
      "source": [
        "len(train_data), len(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3pbPn4nkIlW"
      },
      "source": [
        "trial_sentence, trial_tags = train_data[42]\n",
        "show_markup(trial_sentence, trial_tags, PALETTE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLelRzOnkIlW"
      },
      "source": [
        "## Tag statistics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CobUTMJpkIlY"
      },
      "source": [
        "word_counter, labels = form_vocabulary_and_tagset(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJhUZ3mlkIlY"
      },
      "source": [
        "for label in labels:\n",
        "    num_words = len(word_counter[label])\n",
        "    print(f'{num_words} \\twords in {label}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZFRddq4kIlZ"
      },
      "source": [
        "word_counter['I-Comfort'].most_common()[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDU2mBm2kIla"
      },
      "source": [
        "## Downloading [FastText model](https://rusvectores.org/ru/models/) pretrained on Taiga corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irxx-Y34kIlb"
      },
      "source": [
        "!wget \"http://vectors.nlpl.eu/repository/20/187.zip\" -P \"/tmp\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eM1XwYz4kIlc"
      },
      "source": [
        "with zipfile.ZipFile('/tmp/187.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/tmp/187/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NQ3dxZYkIlc"
      },
      "source": [
        "w2v_model = gensim.models.KeyedVectors.load('/tmp/187/model.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCU4gRSrkIlb"
      },
      "source": [
        "## Check it on rare and invented words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbsVV9zLkIlc"
      },
      "source": [
        "words = ['тачаночка', 'двиганчик', 'ковервертолет']\n",
        "\n",
        "for word in words:\n",
        "    print(word)\n",
        "    for i in w2v_model.most_similar(positive=[word], topn=10):\n",
        "        print(i)\n",
        "        nearest_word, cosine_similarity = i[0], i[1]\n",
        "        print(nearest_word, cosine_similarity)\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpB4ky1NkIle"
      },
      "source": [
        "## Now can [convert](#scrollTo=bD-86JWd9bCO&line=1&uniqifier=1) words to FastText embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m14rb0gkIlf"
      },
      "source": [
        "word_converter = WordConverter(w2v_model)\n",
        "\n",
        "words, _ = train_data[42]\n",
        "vectors = word_converter.convert(words)\n",
        "\n",
        "vectors.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1JQRJk7kIla"
      },
      "source": [
        "tag_converter = TagConverter(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR7kso6xtkoO"
      },
      "source": [
        "## Creating a [torch dataset](#scrollTo=7YKa1NM6nPvF&line=1&uniqifier=1):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I1wdplztj0-"
      },
      "source": [
        "train_dataset = TaggerDataset(train_data, word_converter, tag_converter)\r\n",
        "val_dataset = TaggerDataset(test_data, word_converter, tag_converter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVj9Xl3o4pG6"
      },
      "source": [
        "## Weights for the loss function:\r\n",
        "### $ \\omega_{class} =  L2Norm(\\#elements\\_in\\_classes) / \\#elements\\_in\\_class $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBt6x8o2kIlg"
      },
      "source": [
        "tag_counter = Counter()\n",
        "for label in labels:\n",
        "    tag_counter[label] = len(word_counter[label])\n",
        "\n",
        "weights = torch.zeros(len(tag_counter) + 1) # + 1 because of zero-class for padding\n",
        "class_counts = torch.ones(len(tag_counter))\n",
        "\n",
        "for tag, count in tag_counter.most_common():\n",
        "    tag_idx = tag_converter.tags_to_index([tag]) - 1\n",
        "    class_counts[tag_idx] = count\n",
        "\n",
        "norm = LA.norm(class_counts, 2)\n",
        "weights[1:] = norm.expand_as(class_counts).div(class_counts)\n",
        "\n",
        "print(\"Weights (zero is a padding class):\\n\", weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pn1P9_3aRm7H"
      },
      "source": [
        "## Finding the appropriate batch sizes for train and validation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FR78hgekRoey"
      },
      "source": [
        "print(f\"divisors of train dataset size ({len(train_dataset)}) are {divisors(len(train_dataset))}\")\r\n",
        "print(f\"divisors of val dataset size ({len(val_dataset)}) are {divisors(len(val_dataset))}\")\r\n",
        "\r\n",
        "batch_size_train = 111\r\n",
        "batch_size_val = 662"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGUgGR21RuK4"
      },
      "source": [
        "## [LSTM model](#scrollTo=uKYfPMwyNvPW&line=1&uniqifier=1):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSIHrBeQkIlf"
      },
      "source": [
        "TAGSET_SIZE   = len(tag_converter.tag_to_idx) + 1\n",
        "EMBEDDING_DIM = w2v_model.vector_size\n",
        "HIDDEN_DIM    = 400\n",
        "NUM_LAYERS    = 1\n",
        "LAYER_DROPOUT = 0.5\n",
        "EMB_DROPOUT   = 0.85"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeDqiakxkIlh"
      },
      "source": [
        "model = LSTMTagger(TAGSET_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS, LAYER_DROPOUT, EMB_DROPOUT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDmL29EJR9-9"
      },
      "source": [
        "## [Training](#scrollTo=HBG5We9PyXB5&line=1&uniqifier=1):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-_HI1bsQwiG"
      },
      "source": [
        "lr_scheduler = lambda optim: \\\r\n",
        "    torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\r\n",
        "\r\n",
        "#lr_scheduler = lambda optim: \\\r\n",
        "#    torch.optim.lr_scheduler.StepLR(optim, step_size=5, gamma=0.9)\r\n",
        "\r\n",
        "best_val_loss, best_model = train_eval_loop(model,\r\n",
        "                                            train_dataset,\r\n",
        "                                            val_dataset,\r\n",
        "                                            batch_nllloss,\r\n",
        "                                            weights=weights,\r\n",
        "                                            lr=1e-3,\r\n",
        "                                            epoch_n=1000,\r\n",
        "                                            batch_size_train=batch_size_train,\r\n",
        "                                            batch_size_val=batch_size_val,\r\n",
        "                                            early_stopping_patience=20,\r\n",
        "                                            lr_scheduler_ctor=lr_scheduler,\r\n",
        "                                            draw_loss=False,\r\n",
        "                                            show_bar=True,\r\n",
        "                                            dataloader_workers_n=5)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkb0Pd6KcDJe"
      },
      "source": [
        "## Some examples of model work:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VqbKBHQcMHj"
      },
      "source": [
        "size_for_demonstrate = 5\r\n",
        "demonstrative_dataset = Subset(val_dataset, indices=range(size_for_demonstrate))\r\n",
        "demonstrative_tags_pred = predict_tags(best_model, demonstrative_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_F9uD8WcMl_"
      },
      "source": [
        "for i in range(size_for_demonstrate):\r\n",
        "    (item, _, _), tags = demonstrative_dataset[i]\r\n",
        "    test_sentence, _ = test_data[item]\r\n",
        "\r\n",
        "    print('Ground truth')\r\n",
        "    show_markup(test_sentence, tag_converter.indices_to_tags(tags), PALETTE)\r\n",
        "\r\n",
        "    print('Prediction:')\r\n",
        "    show_markup(test_sentence, tag_converter.indices_to_tags(demonstrative_tags_pred[i]), PALETTE)\r\n",
        "    \r\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzvugfZ07jX0"
      },
      "source": [
        "## Prediction and confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYh1zM8Em5B4"
      },
      "source": [
        "val_tags_pred = predict_tags(best_model, val_dataset, device=None, batch_size=662)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmBsByulmvF_"
      },
      "source": [
        "y_pred, y_true = [], []\r\n",
        "\r\n",
        "for i in range(len(val_dataset)):\r\n",
        "    (_, _, sentence_len), tags = val_dataset[i]\r\n",
        "    y_pred += tag_converter.indices_to_tags(val_tags_pred[i])\r\n",
        "    y_true += tag_converter.indices_to_tags(tags[:sentence_len])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFE1I7N-kIll"
      },
      "source": [
        "plot_confusion_matrix(y_true, y_pred, classes=tag_converter.idx_to_tag, figsize=(10,10), normalize=True, \r\n",
        "                      title='Normalized confusion matrix')\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}